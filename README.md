# ITV Data Scientist task
Repository for my submission to the ITV Data Scientist task

## Running the notebook
The csv file required for the two sections is contained in the repository.

A jupyter notebook is supplied with code to answer parts 1 & 2 of the tast. The notebook should run (providing libraries required have been installed). An html file of the notebook is also provided so the code and output can be viewed in case there was any issues with running the notebook.

## Part 1

Contains answers to the data wrangling questions

## Part 2 - Predicting Lifetime Post Consumers

In order to predict lifetime post consumers 3 models was chosen to model the data (linear regression and regression with ridge and lasso regularisation). In order to use linear regression a series of assumption are required:

- Linear relationship
- Multivariate normality
- No or little multicollinearity
- No auto-correlation
- Homoscedasticity

#### Pre processing of data

In order to achieve the multivariate normality the target variable is log transformed (due to the positive skew) along with removing some of the outliers (deemed anything more than 3 sd away from the mean). 

There was one intance of missing value in the paid column which was treated. 

Page total likes was removed from being used in the model due to the strong correlation with month of post (page total likes increased as month increased).

#### Feature Engineering

A small amount of feature engineering was conducted. The total page likes since last post was calculated from the total page likes information. Whether the post was a single entry on a particular day or part of multiple posts was also calculated. More feature engineering would have been conducted with a bigger data set or with more time and some of the ideas around that are specified in the end of the notebook.

#### Model Performance

All 3 models show similar cross validation and test set accuracies, with Lasso regression performing the best (0.429 MSE for CV and 0.434 MSE for test). This demonstrates that the mode is suffering from high variance (overfitting on training data and not performing well on test set). An R2 score of 0.45 is achieved using the lasso regression model and although a value of close to 1 would be desired on the small data set and as a baseline model this value feels acceptable, although there could be a lot more added and tested to improve that score further. 

There is litte multicollinearity between variables and the plot of residuals from the model output show there is no auto-correlation and the residuals are homoscedastic. This is a good sign that the model built is following the assumptions required for linear regression.

#### Next steps with more time

- More feature engineering: Due to the size of the data and unable to tell when in a month a particular post was made (there were obvious gaps between days of week that meant it wasn't obvious to tell when a post was made) no features relating to posts made in the last couple of weeks or last month could be provided. I would want to look into some of the posts that had the highest error between actual and predicted lifetime post consumers and see if they gave any indication as to why the model couldn't pick that up from the data provided.

- Test other models: Only linear regression has been touched upon so far to build a baseline model. Other models would also need to be tested as they might be able to uncover patterns not picked up in the linear models and produce better predictions.

- Productionalise the model: If the predictions generated by the model were needed often (say to predict the imapct of posting at a certain time or on a certain day in the week) an API could be created that would allow the trained model to be used to provide predictions if a user called the API (for example through a GUI). A package such as flask could be used to create the API.
